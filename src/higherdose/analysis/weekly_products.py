#!/usr/bin/env python3
"""
HigherDOSE Weekly Product & Category Growth Report

This script extends the channel-level weekly report by mapping each campaign/ad
row to a specific product (and its category) based on the naming found in the
Ad Name ‚Üí Ad Set Name ‚Üí Campaign Name cascade.  It then outputs additional
markdown tables summarising performance by product and by product category.

The original channel-level analytics remain intact via re-use of
`report_analysis_weekly` helper functions.
"""
import ast
import re
from datetime import datetime, timedelta

import numpy as np
import pandas as pd

from . import weekly as base
import glob, os
import argparse
from pathlib import Path
from higherdose.utils.io.file_selector import select_csv_file


# -------------------------------------------------------------
# üìÖ  Previous Week Utilities
# -------------------------------------------------------------

# Helper to pick newest file matching a pattern (used as fallback)
def _latest(pattern: str) -> str | None:
    matches = glob.glob(pattern)
    return max(matches, key=os.path.getmtime) if matches else None


def _find_previous_csv(stats_dir: str = "data/ads") -> str | None:
    """Return path to the previous-week CSV.

    Priority:
    1. Any file whose basename starts with ``prev-`` (Northbeam export you drop in).
    2. Otherwise, the *second* most-recent CSV in the directory.
    """
    csvs = sorted(glob.glob(os.path.join(stats_dir, "*.csv")), key=os.path.getmtime, reverse=True)
    for p in csvs:
        if os.path.basename(p).startswith("prev-"):
            return p
    if len(csvs) >= 2:
        return csvs[1]
    return None


def _load_csv_clean(path: str) -> pd.DataFrame | None:
    """Lightweight clone of base.load_and_clean_data() but non-interactive."""
    if not path or not os.path.exists(path):
        print("‚ö†Ô∏è  Previous-week CSV not found ‚Äì skipping deltas")
        return None
    df = pd.read_csv(path)

    # Minimal numeric cast
    numeric_cols = [
        "spend",
        "attributed_rev",
        "attributed_rev_1st_time",
        "transactions",
        "transactions_1st_time",
        "visits",
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.fillna(0)
    # Ensure required platform col present
    if "breakdown_platform_northbeam" not in df.columns:
        df["breakdown_platform_northbeam"] = df.get("platform", "Unknown")
    return df


# -------------------------------------------------------------
# Œî  Delta Formatting Helpers
# -------------------------------------------------------------


def _pct_delta(cur: float, prev: float) -> float:
    if prev == 0:
        return 0.0
    return (cur - prev) / prev * 100.0


def _fmt_delta(cur: float, prev: float, prefix: str = "$", digits: int = 0) -> str:
    """Return a formatted cell showing cur, prev and %Œî."""
    pct = _pct_delta(cur, prev)
    sign = "+" if pct > 0 else ("-" if pct < 0 else "")
    pct_str = f"{sign}{abs(pct):.0f}%"
    if prefix:
        cur_str = f"{prefix}{cur:,.{digits}f}"
        prev_str = f"{prefix}{prev:,.{digits}f}"
    else:
        cur_str = f"{cur:,.{digits}f}"
        prev_str = f"{prev:,.{digits}f}"
    return f"{cur_str} ( {prev_str} | {pct_str} )"


# -------------------------------------------------------------
# üîé  Product & Category Mapping Helpers
# -------------------------------------------------------------

def load_product_mappings(md_path: str = "docs/product-list.md"):
    """Parse the product-list markdown to build mappings without importing it as a module.

    We look for literal `categories = {...}` and optional `aliases = {...}` blocks and
    evaluate them safely with `ast.literal_eval` so we avoid executing any other code
    (like the file-writing section).
    """
    with open(md_path, "r", encoding="utf-8") as f:
        content = f.read()

    def _extract_dict(name: str):
        pattern = rf"{name}\s*=\s*(\{{.*?\}})"  # greedy balanced with simple heuristic
        match = re.search(pattern, content, re.S)
        if match:
            try:
                return ast.literal_eval(match.group(1))
            except Exception:
                pass
        return {}

    categories_dict = _extract_dict("categories")
    aliases_dict = _extract_dict("aliases")

    # Build product ‚Üí category mapping
    product_to_category: dict[str, str] = {}
    for cat, prods in categories_dict.items():
        for prod in prods:
            product_to_category[prod] = cat

    # Ensure a default bucket for unmatched rows
    product_to_category.setdefault("Unattributed", "Unattributed")

    # Normalize helper
    def _norm(s: str):
        # insert spaces before CamelCase transitions first
        s = re.sub(r"(?<=[a-z0-9])(?=[A-Z])", " ", s)
        s = s.replace("_", " ").replace("-", " ")
        s = s.lower()
        s = re.sub(r"[^a-z0-9 ]+", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    alias_map = {_norm(k): v for k, v in aliases_dict.items()}
    # Ensure canonical names map to themselves
    for prod in product_to_category:
        alias_map.setdefault(_norm(prod), prod)

    # Precompute variants with spaces removed for camel-case matches
    expanded_alias = {}
    for key, val in alias_map.items():
        expanded_alias[key] = val
        nospace = key.replace(" ", "")
        if nospace != key:
            expanded_alias.setdefault(nospace, val)

    # sort longest->shortest for deterministic greedy matching
    alias_sorted = sorted(expanded_alias.items(), key=lambda kv: len(kv[0]), reverse=True)
    return product_to_category, alias_sorted, _norm


def detect_product(row, alias_sorted, norm_fn):
    """Return canonical product found in Ad Name, Ad Set, then Campaign."""
    search_fields = ("ad_name", "adset_name", "campaign_name")
    for field in search_fields:
        original = str(row.get(field, ""))
        text_norm = norm_fn(original)
        text_nospace = text_norm.replace(" ", "")
        for alias, canonical in alias_sorted:
            if alias in text_norm or alias in text_nospace:
                return canonical
    return None


def assign_products(df: pd.DataFrame, alias_sorted, norm_fn):
    df = df.copy()
    df["product"] = df.apply(lambda r: detect_product(r, alias_sorted, norm_fn), axis=1)
    return df


def build_summary(df: pd.DataFrame, group_col: str):
    numeric_cols = [
        "spend",
        "attributed_rev",
        "attributed_rev_1st_time",
        "transactions",
        "transactions_1st_time",
    ]
    present = [c for c in numeric_cols if c in df.columns]
    summary = df.groupby(group_col)[present].sum().astype(float)

    # Metric calculations
    summary["roas"] = (summary["attributed_rev"] / summary["spend"]).replace([np.inf], 0)
    summary["roas_1st_time"] = (
        summary["attributed_rev_1st_time"] / summary["spend"]
    ).replace([np.inf], 0)
    # Use *rounded* transactions for CAC / AOV to avoid confusing fractional counts
    txns_rounded = summary["transactions"].round().replace(0, np.nan)
    summary["cac"] = (summary["spend"] / txns_rounded).replace([np.inf], 0).fillna(0)
    summary["cac_1st_time"] = (
        summary["spend"] / summary["transactions_1st_time"].round().replace(0, np.nan)
    ).replace([np.inf], 0).fillna(0)
    summary["aov"] = (
        summary["attributed_rev"] / txns_rounded
    ).replace([np.inf], 0).fillna(0)

    # Store the rounded value for display purposes
    summary["transactions_display"] = txns_rounded.fillna(0)

    summary = summary.replace([np.inf, -np.inf], 0).round(2)
    summary = summary.sort_values("spend", ascending=False)
    return summary


def markdown_table(
    summary: pd.DataFrame,
    index_label: str,
    extra_col: str | None = None,
    prev_summary: pd.DataFrame | None = None,
):
    """Return a markdown table with optional Prev and Œî% columns for each metric."""

    headers: list[str] = [index_label]
    if extra_col:
        headers.append(extra_col)

    metrics = [
        ("spend", "$", 0, "Spend"),
        ("attributed_rev", "$", 0, "Revenue"),
        ("roas", "", 2, "ROAS"),
        ("roas_1st_time", "", 2, "ROAS 1st"),
        ("cac", "$", 2, "CAC"),
        ("cac_1st_time", "$", 2, "CAC 1st"),
        ("aov", "$", 2, "AOV"),
        ("transactions_display", "", 0, "Transactions"),
    ]

    for m, _, _, title in metrics:
        headers.append(title)
        if prev_summary is not None:
            headers.extend([f"{title} Prev", f"{title} Œî%"])

    lines = ["| " + " | ".join(headers) + " |", "|" + "|".join(["-"] * len(headers)) + "|"]

    for idx, row in summary.iterrows():
        cells: list[str] = [idx]
        if extra_col:
            cells.append(str(row[extra_col]))

        for metric, prefix, dec, _ in metrics:
            cur_val = row[metric] if metric in row else 0
            # Current value
            cur_fmt = f"{prefix}{cur_val:,.{dec}f}" if prefix else f"{cur_val:,.{dec}f}" if dec else f"{int(cur_val)}"
            cells.append(cur_fmt)

            if prev_summary is not None:
                prev_val = prev_summary.loc[idx][metric] if idx in prev_summary.index and metric in prev_summary.columns else 0
                prev_fmt = f"{prefix}{prev_val:,.{dec}f}" if prefix else f"{prev_val:,.{dec}f}" if dec else f"{int(prev_val)}"
                pct = _pct_delta(cur_val, prev_val)
                sign = "+" if pct > 0 else ("-" if pct < 0 else "")
                delta_fmt = f"{sign}{abs(pct):.0f}%"
                cells.extend([prev_fmt, delta_fmt])

        lines.append("| " + " | ".join(cells) + " |")

    return "\n".join(lines)


# -------------------------------------------------------------
# Œ£  Totals row helper specifically for CHANNEL table so that
#   WoW Prev and Œî columns can be displayed.
# -------------------------------------------------------------


def channel_totals_df(summary: pd.DataFrame, label: str = "**All Channels**") -> pd.DataFrame:
    """Return 1-row DataFrame with totals and rounded txns_display."""
    spend = summary["spend"].sum()
    revenue = summary["attributed_rev"].sum()
    txns = summary["transactions"].sum()

    txns_display = round(txns)
    rev_1st_sum = summary["attributed_rev_1st_time"].sum() if "attributed_rev_1st_time" in summary.columns else 0
    txns_1st_sum = summary["transactions_1st_time"].sum() if "transactions_1st_time" in summary.columns else 0
    row = {
        "spend": spend,
        "attributed_rev": revenue,
        "roas": revenue / spend if spend else 0,
        "roas_1st_time": rev_1st_sum / spend if spend else 0,
        "cac": spend / txns_display if txns_display else 0,
        "cac_1st_time": spend / txns_1st_sum if txns_1st_sum else 0,
        "aov": revenue / txns_display if txns_display else 0,
        "transactions": txns_display,
        "transactions_display": txns_display,
    }
    return pd.DataFrame(row, index=[label])


# -------------------------------------------------------------
# Œ£  Helper to add totals row
# -------------------------------------------------------------

def totals_row(summary: pd.DataFrame, label: str):
    """Return a one-row DataFrame with totals across the provided summary."""
    numeric_base = [
        "spend",
        "attributed_rev",
        "transactions",
    ]

    agg = summary[numeric_base].sum()
    spend = agg["spend"]
    revenue = agg["attributed_rev"]
    txns = agg["transactions"]

    txns_display = round(txns)

    rev_1st_sum = summary["attributed_rev_1st_time"].sum() if "attributed_rev_1st_time" in summary.columns else 0
    txns_1st_sum = summary["transactions_1st_time"].sum() if "transactions_1st_time" in summary.columns else 0
    total_series = {
        "spend": spend,
        "attributed_rev": revenue,
        "roas": revenue / spend if spend else 0,
        "roas_1st_time": rev_1st_sum / spend if spend else 0,
        "cac": spend / txns_display if txns_display else 0,
        "cac_1st_time": spend / txns_1st_sum if txns_1st_sum else 0,
        "aov": revenue / txns_display if txns_display else 0,
        "transactions": txns_display,
        "transactions_display": txns_display,
    }

    return pd.DataFrame(total_series, index=[label])


# -------------------------------------------------------------
# üèÅ  Main Routine
# -------------------------------------------------------------

def main():
    print("HigherDOSE Weekly Product/Category Report")
    print("========================================\n")

    # -------------------------------------------------------------
    # üîß  CLI arguments (paths to current-year MTD CSVs)
    # -------------------------------------------------------------

    parser = argparse.ArgumentParser(description="Generate weekly growth report with YoY comparisons")
    parser.add_argument("--google_csv", help="Path to current Google Ads MTD CSV", default=None)
    parser.add_argument("--meta_csv", help="Path to current Meta Ads MTD CSV", default=None)
    args, _unknown = parser.parse_known_args()

    # Determine current-year MTD files (CLI arg if supplied, otherwise prompt)
    def _prompt_mtd(platform: str, cli_value: str | None):
        if cli_value:
            return cli_value
        pattern = f"*{platform.lower()}*mtd*csv"
        return select_csv_file(
            directory="data/ads",
            file_pattern=pattern,
            prompt_message=f"Select {platform} MTD CSV (or q to skip): ",
            max_items=10,
        )

    google_cur_path = _prompt_mtd("google", args.google_csv)
    meta_cur_path   = _prompt_mtd("meta",   args.meta_csv)

    # 1. Load channel-level cleaned data from the base module
    df = base.load_and_clean_data()
    if df is None:
        return

    # Preserve a copy of the *full* dataset before we potentially
    # slice it down to a specific date range so we can use it to
    # derive the previous-period DataFrame without re-prompting for
    # a file.
    df_full = df.copy()

    # -------------------------------------------------------------
    # üìÖ  Interactive date filtering (new)
    # -------------------------------------------------------------
    # If the loaded CSV now contains an explicit date column, allow
    # the user to choose a custom date window (e.g. the most recent
    # 7-day period) and automatically derive the *previous* period
    # for WoW comparisons from the *same* file.  If no usable date
    # column is found we fall back to the older behaviour that looks
    # for a separate ‚Äúprev-‚Äù CSV on disk.
    # -------------------------------------------------------------

    # Identify potential date column(s) ‚Äì case-insensitive match
    date_cols = [c for c in df.columns if c.lower() in {"date", "day", "report_date"}]
    use_date_filter = bool(date_cols)

    prev_df: pd.DataFrame | None = None  # will be populated below

    if use_date_filter:
        date_col = date_cols[0]
        # Ensure datetime dtype for both filtered and full copies
        df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
        df_full[date_col] = pd.to_datetime(df_full[date_col], errors="coerce")

        # --------------------------------------------------
        # ‚ë†  Select CURRENT period
        # --------------------------------------------------
        period_menu = (
            "Choose reporting period (Northbeam alignment):\n"
            "  1) Last 7 days\n"
            "  2) Last 14 days\n"
            "  3) Last 30 days\n"
            "  4) Custom range\n"
        )
        while True:
            choice = input(period_menu + "Selection: ").strip()
            if choice in {"1", "7", "last 7", "l7"}:
                days = 7
            elif choice in {"2", "14", "last 14", "l14"}:
                days = 14
            elif choice in {"3", "30", "last 30", "l30"}:
                days = 30
            elif choice in {"4", "custom", "c"}:
                days = None  # handled below
            else:
                print("‚ùå Invalid option ‚Äì try again.\n")
                continue
            break

        if days is not None:
            # Northbeam presets end on *yesterday*, not today
            latest_dt = df_full[date_col].max().date()
            end_date = latest_dt - timedelta(days=1)
            start_date = end_date - timedelta(days=days - 1)
        else:
            # Custom range prompt
            while True:
                try:
                    start_input = input("Enter report START date  (YYYY-MM-DD): ").strip()
                    end_input   = input("Enter report END date    (YYYY-MM-DD): ").strip()
                    start_date = pd.to_datetime(start_input).date()
                    end_date   = pd.to_datetime(end_input).date()
                    if end_date < start_date:
                        print("‚ùå End date must not be before start date. Try again.\n")
                        continue
                    break
                except Exception:
                    print("‚ùå Invalid date format ‚Äì please use YYYY-MM-DD.\n")

        # Slice current period
        df = df[(df[date_col].dt.date >= start_date) & (df[date_col].dt.date <= end_date)].copy()

        if df.empty:
            print("‚ö†Ô∏è  No data found in the selected period ‚Äì exiting.")
            return

        # --------------------------------------------------
        # ‚ë°  Select COMPARISON window
        # --------------------------------------------------
        compare_menu = (
            "Compare against:\n"
            "  1) Previous period (same length)\n"
            "  2) Previous month\n"
            "  3) Previous year\n"
            "  4) None\n"
        )
        while True:
            comp_choice = input(compare_menu + "Selection: ").strip()
            if comp_choice == "1":
                prev_start = start_date - timedelta(days=(end_date - start_date).days + 1)
                prev_end   = start_date - timedelta(days=1)
                break
            elif comp_choice == "2":
                prev_start = (start_date - pd.DateOffset(months=1)).date()
                prev_end   = (end_date   - pd.DateOffset(months=1)).date()
                break
            elif comp_choice == "3":
                prev_start = (start_date - pd.DateOffset(years=1)).date()
                prev_end   = (end_date   - pd.DateOffset(years=1)).date()
                break
            elif comp_choice == "4":
                prev_start = prev_end = None
                break
            else:
                print("‚ùå Invalid option ‚Äì try again.\n")

        if prev_start and prev_end:
            prev_df_subset = df_full[(df_full[date_col].dt.date >= prev_start) & (df_full[date_col].dt.date <= prev_end)].copy()
            if not prev_df_subset.empty:
                prev_df = prev_df_subset.fillna(0)

        # Announce selection summary
        label_prev = f"{prev_start} ‚Üí {prev_end}" if prev_start else "NONE"
        print(f"üìÖ Current period : {start_date} ‚Üí {end_date}\n" +
              f"üîÅ Comparison     : {label_prev}\n")
    # -------------------------------------------------------------
    # Legacy behaviour ‚Äì fall back to separate previous-week CSV
    # -------------------------------------------------------------
    else:
        prev_csv = _find_previous_csv()
        prev_df = _load_csv_clean(prev_csv) if prev_csv else None

    # -------------------------------------------------------------
    # 2Ô∏è‚É£  Product mapping & summary preparation (current period)
    # -------------------------------------------------------------

    product_to_category, alias_sorted, norm_fn = load_product_mappings()

    # Assign canonical product to each row
    df_prod = assign_products(df, alias_sorted, norm_fn)

    # Label rows with no matched product as 'Unattributed'
    df_prod["product"] = df_prod["product"].fillna("Unattributed")

    # --- Accrual rows (for unattributed + meta grouping logic later) ---
    accrual_df_prod = df_prod[df_prod["accounting_mode"] == "Accrual performance"].copy()

    def _is_blank(val):
        return pd.isna(val) or str(val).strip() == "" or str(val).strip().lower() == "(no name)"

    # Flag campaign-summary rows (blank adset/ad names) and drop them when detailed rows exist
    accrual_df_prod["_is_summary"] = accrual_df_prod.apply(
        lambda r: _is_blank(r["adset_name"]) and _is_blank(r["ad_name"]), axis=1
    )
    has_detail = accrual_df_prod.groupby("campaign_name")["_is_summary"].transform(lambda s: (~s).any())
    accrual_filtered = accrual_df_prod[~(accrual_df_prod["_is_summary"] & has_detail)].copy()

    # --- Cash snapshot rows for CAC/AOV consistency ---
    cash_df_prod = df_prod[df_prod["accounting_mode"] == "Cash snapshot"].copy()
    cash_df_prod["_is_summary"] = cash_df_prod.apply(
        lambda r: _is_blank(r["adset_name"]) and _is_blank(r["ad_name"]), axis=1
    )
    cash_has_detail = cash_df_prod.groupby("campaign_name")["_is_summary"].transform(lambda s: (~s).any())
    cash_filtered = cash_df_prod[~(cash_df_prod["_is_summary"] & cash_has_detail)].copy()

    # Build summaries
    product_summary = build_summary(cash_filtered[cash_filtered["product"].notna()], "product")

    cash_filtered["category"] = cash_filtered["product"].map(product_to_category).fillna("Unattributed")
    category_summary = build_summary(cash_filtered[cash_filtered["category"].notna()], "category")

    # -------------------------------------------------------------
    # Previous-period mapping placeholders (will be filled if prev_df exists)
    # -------------------------------------------------------------

    prev_product_summary = prev_category_summary = None

    if prev_df is not None:
        prev_df_prod = assign_products(prev_df, alias_sorted, norm_fn)
        prev_cash_df = prev_df_prod[prev_df_prod["accounting_mode"] == "Cash snapshot"].copy()
        prev_cash_df["category"] = prev_cash_df["product"].map(product_to_category).fillna("Unattributed")

        prev_product_summary = build_summary(prev_cash_df[prev_cash_df["product"].notna()], "product")
        prev_category_summary = build_summary(prev_cash_df[prev_cash_df["category"].notna()], "category")

    # -------------------------------------------------------------
    # üö© Capture Unattributed rows for alias discovery
    # -------------------------------------------------------------
    unattributed_df = accrual_filtered[accrual_filtered["product"] == "Unattributed"].copy()

    if not unattributed_df.empty:
        cols_to_keep = [
            "breakdown_platform_northbeam",
            "campaign_name",
            "adset_name",
            "ad_name",
            "spend",
            "attributed_rev",
        ]
        unattributed_export = unattributed_df[cols_to_keep].sort_values("spend", ascending=False)

        out_dir = Path("data/products/unattributed")
        out_dir.mkdir(parents=True, exist_ok=True)
        export_name = out_dir / f"unattributed_lines_{datetime.now().strftime('%Y-%m-%d')}.csv"
        unattributed_export.to_csv(export_name, index=False)
        print(f"üì§ Exported {len(unattributed_export)} unattributed rows to {export_name}")

    # 4. Run existing channel-level analyses (for executive summary etc.)
    channel_summary = base.analyze_channel_performance(df)
    if channel_summary.empty:
        print("No channel data ‚Äì exiting")
        return

    executive_metrics = base.generate_executive_summary(channel_summary)
    campaign_analysis, revenue_only_df = base.analyze_campaign_performance(df)
    first_time_metrics = base.analyze_first_time_metrics(df)
    base.analyze_attribution_modes(df)
    base.identify_opportunities(channel_summary)

    # 5. Assemble markdown report
    base_report = base.export_markdown_report(
        executive_metrics,
        channel_summary,
        campaign_analysis,
        revenue_only_df,
        first_time_metrics,
    )

    # Prepare product summary with Category column for display
    # Product display with totals row
    prod_display = product_summary.copy()
    prod_display["Category"] = prod_display.index.map(product_to_category)

    prod_tot = totals_row(prod_display, label="**All Products**")
    prod_tot["Category"] = "‚Äî"

    prod_table_df = pd.concat([prod_tot, prod_display])

    # Category display with totals row
    cat_tot = totals_row(category_summary, label="**All Categories**")
    category_table_df = pd.concat([cat_tot, category_summary])

    product_section_md = (
        "\n## 2a. Performance by Product (Cash Snapshot)\n" +
        markdown_table(prod_table_df, index_label="Product", extra_col="Category") +
        "\n\n## 2b. Performance by Category (Cash Snapshot)\n" +
        markdown_table(category_table_df, index_label="Category") +
        "\n---\n"
    )

    # -------------------------------------------------------------
    # üöÄ Meta Product Group Performance (Accrual Performance)
    # -------------------------------------------------------------
    meta_base = df[(df["accounting_mode"] == "Accrual performance") &
                   (df["breakdown_platform_northbeam"].astype(str).str.contains(r"meta|facebook|fb|instagram", case=False, na=False))].copy()

    # Remove campaign summary rows to avoid double-counting (same logic as above)
    def _is_blank(val):
        return pd.isna(val) or str(val).strip() == "" or str(val).strip().lower() == "(no name)"

    meta_base["_is_summary"] = meta_base.apply(
        lambda r: _is_blank(r["adset_name"]) and _is_blank(r["ad_name"]), axis=1
    )
    has_meta_detail = meta_base.groupby("campaign_name")["_is_summary"].transform(lambda s: (~s).any())
    meta_accrual = meta_base[~(meta_base["_is_summary"] & has_meta_detail)].copy()

    if not meta_accrual.empty:
        # Use campaign-name keywords for grouping to align with user‚Äôs totals
        def _keyword_group(row):
            text = str(row.get("campaign_name", "")).lower()
            # Explicit keywords checked in priority order to avoid mis-classification
            if "bundle" in text:
                return "Bundle"
            if "blanket" in text:
                return "Sauna Blanket"
            if "pemf" in text:
                return "PEMF Mat"
            if "hat" in text:
                return "Red Light Hat"
            if "mask" in text:
                return "Red Light Mask"
            return "Body Care & Supplements"

        meta_accrual["product_group"] = meta_accrual.apply(_keyword_group, axis=1)

        # Ensure groups exactly match the desired order for table output
        desired_order = [
            "Body Care & Supplements",
            "Sauna Blanket",
            "Red Light Hat",
            "PEMF Mat",
            "Red Light Mask",
            "Bundle",
        ]

        meta_group_df = meta_accrual.copy()

        meta_summary = build_summary(meta_group_df, "product_group")

        # Re-index to ensure all groups appear (missing groups will be 0)
        meta_summary = meta_summary.reindex(desired_order).fillna(0)

        # Build markdown table mirroring DTC metrics
        headers = [
            "Product Group",
            "Spend",
            "% of Total",
            "CAC",
            "CAC 1st",
            "ROAS",
            "ROAS 1st",
            "AOV",
            "Transactions",
            "Revenue",
        ]
        meta_lines = ["| " + " | ".join(headers) + " |", "|" + "|".join(["-"] * len(headers)) + "|"]

        total_spend_meta = meta_summary["spend"].sum() if not meta_summary.empty else 0

        for idx, row in meta_summary.iterrows():
            spend = row["spend"]
            pct_total = (spend / total_spend_meta * 100) if total_spend_meta else 0
            meta_lines.append(
                f"| {idx} | ${spend:,.0f} | {pct_total:.1f}% | "
                f"${row['cac']:,.2f} | ${row['cac_1st_time']:,.2f} | "
                f"{row['roas']:.2f} | {row['roas_1st_time']:.2f} | "
                f"${row['aov']:,.0f} | {int(row['transactions_display'])} | "
                f"${row['attributed_rev']:,.0f} |"
            )

        meta_section_md = (
            "\n## 2c. Meta Product Group Performance (Accrual Performance)\n" +
            "\n".join(meta_lines) +
            "\n---\n"
        )
    else:
        meta_section_md = ""

    # -------------------------------------------------------------
    # üîÑ Build DTC channel table with WoW columns
    # -------------------------------------------------------------
    # Only keep channels with spend > 0 for display, but compute the
    # totals row using the full data so revenue-only rows are still
    # included in the aggregate metrics.

    channel_summary["transactions_display"] = channel_summary["transactions"].round()

    # Build display DF: totals row + channels with spend > 0
    chan_tot = channel_totals_df(channel_summary)
    channel_nonzero = channel_summary[channel_summary["spend"] > 0].copy()
    # Insert a consolidated **Paid Media** row (all channels with spend)
    paid_tot = totals_row(channel_nonzero, label="**Paid Media**")
    channel_summary_display = pd.concat([chan_tot, paid_tot, channel_nonzero])

    prev_channel_summary = None
    if prev_df is not None:
        prev_channel_summary_full = base.analyze_channel_performance(prev_df)
        if isinstance(prev_channel_summary_full, pd.DataFrame) and not prev_channel_summary_full.empty:
            prev_channel_summary_full["transactions_display"] = prev_channel_summary_full["transactions"].round()
            prev_tot = channel_totals_df(prev_channel_summary_full)
            prev_nonzero = prev_channel_summary_full[prev_channel_summary_full["spend"] > 0].copy()
            prev_paid_tot = totals_row(prev_nonzero, label="**Paid Media**")
            prev_channel_summary = pd.concat([prev_tot, prev_paid_tot, prev_nonzero])

    dtc_table_md = "## 2. DTC Breakdown (Accrual Performance) - 7 Days (Northbeam)\n" + \
        markdown_table(channel_summary_display, index_label="Channel", prev_summary=prev_channel_summary) + "\n"

    # ----------------------------------------------
    # üìù  Build the final markdown by starting from
    #     the base report, renaming the DTC header,
    #     and inserting the Product & Category tables
    #     directly beneath that section (as 2a/2b).
    # ----------------------------------------------

    final_report = base_report  # start fresh from base

    # 1) Rename key section headers so we can reliably locate them
    header_map = {
        "## 2. DTC Performance ‚Äî 7-Day Snapshot (Northbeam)": "## 2. DTC Breakdown (Accrual Performance) - 7 Days (Northbeam)",
        "## 3. Top Campaign Performance Analysis": "## 3. Top Campaign Performance Analysis (Accrual Performance)",
        "## 4. Channel Performance Metrics": "## 4. Channel Performance Metrics (Accrual Performance)",
    }
    for old, new in header_map.items():
        final_report = final_report.replace(old, new)

    # 2) Replace the existing DTC table with the enhanced version that
    #    includes WoW deltas, then append the Product (2a) & Category (2b)
    #    tables directly afterwards.
    dtc_header = "## 2. DTC Breakdown (Accrual Performance) - 7 Days (Northbeam)"
    hdr_pos = final_report.find(dtc_header)
    if hdr_pos != -1:
        divider_pos = final_report.find("\n---\n", hdr_pos)
        if divider_pos != -1:
            final_report = (
                final_report[:hdr_pos] +
                dtc_table_md + "\n" + product_section_md + meta_section_md +
                final_report[divider_pos:]
            )
        else:
            # Fallback ‚Äì no divider found; replace from header line to next blank line
            next_break = final_report.find("\n\n", hdr_pos)
            replace_end = next_break if next_break != -1 else hdr_pos
            final_report = (
                final_report[:hdr_pos] +
                dtc_table_md + "\n" + product_section_md + meta_section_md +
                final_report[replace_end:]
            )
    else:
        # If header somehow missing, append both sections to ensure they appear.
        final_report += "\n" + dtc_table_md + "\n" + product_section_md + meta_section_md

    # -------------------------------------------------------------
    # üìà Week-over-Week Executive Delta Overview (re-injected)
    # -------------------------------------------------------------
    if prev_df is not None:
        cur_acc = df[df["accounting_mode"] == "Accrual performance"].copy()
        prev_acc = prev_df[prev_df["accounting_mode"] == "Accrual performance"].copy()

        def _totals(d):
            return {
                "spend": d["spend"].sum(),
                "rev": d["attributed_rev"].sum(),
                "txns": d["transactions"].sum(),
                # First-time metrics (may be absent in some exports)
                "rev_1st": d["attributed_rev_1st_time"].sum() if "attributed_rev_1st_time" in d.columns else 0,
                "txns_1st": d["transactions_1st_time"].sum() if "transactions_1st_time" in d.columns else 0,
            }

        tot_cur = _totals(cur_acc)
        tot_prev = _totals(prev_acc)
        # Derived metrics
        tot_cur["roas"] = tot_cur["rev"] / tot_cur["spend"] if tot_cur["spend"] else 0
        tot_prev["roas"] = tot_prev["rev"] / tot_prev["spend"] if tot_prev["spend"] else 0
        # First-time ROAS & CAC
        tot_cur["roas_1st"] = (
            tot_cur["rev_1st"] / tot_cur["spend"] if tot_cur["spend"] else 0
        )
        tot_prev["roas_1st"] = (
            tot_prev["rev_1st"] / tot_prev["spend"] if tot_prev["spend"] else 0
        )
        tot_cur["cac_1st"] = (
            tot_cur["spend"] / tot_cur["txns_1st"] if tot_cur["txns_1st"] else 0
        )
        tot_prev["cac_1st"] = (
            tot_prev["spend"] / tot_prev["txns_1st"] if tot_prev["txns_1st"] else 0
        )

        wow_lines = [
            "\n### Week-over-Week Overview\n",
            f"* **Spend:** {_fmt_delta(tot_cur['spend'], tot_prev['spend'])}",
            f"* **Revenue:** {_fmt_delta(tot_cur['rev'], tot_prev['rev'])}",
            f"* **ROAS:** {_fmt_delta(tot_cur['roas'], tot_prev['roas'], prefix='', digits=2)}",
            f"* **ROAS 1st:** {_fmt_delta(tot_cur['roas_1st'], tot_prev['roas_1st'], prefix='', digits=2)}",
            f"* **CAC 1st:** {_fmt_delta(tot_cur['cac_1st'], tot_prev['cac_1st'], prefix='$', digits=2)}",
            f"* **Transactions:** {_fmt_delta(tot_cur['txns'], tot_prev['txns'], prefix='', digits=0)}",
            "\n",
        ]

        exec_hdr = "## 1. Executive Summary"
        pos_exec = final_report.find(exec_hdr)
        if pos_exec != -1:
            insert_pos = final_report.find("\n", pos_exec + len(exec_hdr)) + 1
            final_report = final_report[:insert_pos] + "\n".join(wow_lines) + final_report[insert_pos:]

    # -------------------------------------------------------------
    # üìä  Year-over-Year Growth Comparison (Google & Meta Ads)
    # -------------------------------------------------------------

    try:
        # Previous-year daily exports (static for now)
        google_prev_path = os.path.join("stats", "google-2024-account-level-daily report.csv")
        meta_prev_path   = os.path.join("stats", "meta-daily-export-jan-1-2024-to-dec-31-2024.csv")

        def _summarize_google(cur_path: str | None, prev_path: str):
            """Return dict with spend, revenue, conversions, roas for current & previous year MTD."""
            if not cur_path or not os.path.exists(cur_path) or not os.path.exists(prev_path):
                return None

            cur_df = pd.read_csv(cur_path, skiprows=2, thousands=",")
            prev_df = pd.read_csv(prev_path, skiprows=2, thousands=",")

            cur_df["Day"] = pd.to_datetime(cur_df["Day"], errors="coerce")
            prev_df["Day"] = pd.to_datetime(prev_df["Day"], errors="coerce")

            if cur_df["Day"].isna().all() or prev_df["Day"].isna().all():
                return None

            end_cur = cur_df["Day"].max()
            start_cur = end_cur - timedelta(days=6)  # Last 7 days inclusive
            end_prev = datetime(end_cur.year - 1, end_cur.month, end_cur.day)
            start_prev = end_prev - timedelta(days=6)

            cur_mtd = cur_df[(cur_df["Day"] >= start_cur) & (cur_df["Day"] <= end_cur)].copy()
            prev_mtd = prev_df[(prev_df["Day"] >= start_prev) & (prev_df["Day"] <= end_prev)].copy()

            for col in ["Cost", "Conv. value", "Conversions"]:
                cur_mtd[col] = pd.to_numeric(cur_mtd[col], errors="coerce")
                prev_mtd[col] = pd.to_numeric(prev_mtd[col], errors="coerce")

            def _tot(df):
                return (
                    df["Cost"].sum(),
                    df["Conv. value"].sum(),
                    df["Conversions"].sum(),
                )

            spend_cur, rev_cur, conv_cur = _tot(cur_mtd)
            spend_prev, rev_prev, conv_prev = _tot(prev_mtd)

            roas_cur = rev_cur / spend_cur if spend_cur else 0
            roas_prev = rev_prev / spend_prev if spend_prev else 0

            cpa_cur = spend_cur / conv_cur if conv_cur else 0
            cpa_prev = spend_prev / conv_prev if conv_prev else 0

            return {
                "spend_cur": spend_cur,
                "spend_prev": spend_prev,
                "rev_cur": rev_cur,
                "rev_prev": rev_prev,
                "conv_cur": conv_cur,
                "conv_prev": conv_prev,
                "roas_cur": roas_cur,
                "roas_prev": roas_prev,
                "cpa_cur": cpa_cur,
                "cpa_prev": cpa_prev,
                "start_date": start_cur.strftime("%B %d"),
                "end_date": end_cur.strftime("%B %d"),
            }

        def _summarize_meta(cur_path: str | None, prev_path: str):
            if not cur_path or not os.path.exists(cur_path) or not os.path.exists(prev_path):
                return None

            cur_df = pd.read_csv(cur_path, thousands=",")
            prev_df = pd.read_csv(prev_path, thousands=",")

            cur_df["Day"] = pd.to_datetime(cur_df["Day"], errors="coerce")
            prev_df["Day"] = pd.to_datetime(prev_df["Day"], errors="coerce")

            if cur_df["Day"].isna().all() or prev_df["Day"].isna().all():
                return None

            end_cur = cur_df["Day"].max()
            start_cur = end_cur - timedelta(days=6)
            end_prev = datetime(end_cur.year - 1, end_cur.month, end_cur.day)
            start_prev = end_prev - timedelta(days=6)

            cur_mtd = cur_df[(cur_df["Day"] >= start_cur) & (cur_df["Day"] <= end_cur)].copy()
            prev_mtd = prev_df[(prev_df["Day"] >= start_prev) & (prev_df["Day"] <= end_prev)].copy()

            for col in ["Amount spent (USD)", "Purchases conversion value", "Purchases"]:
                cur_mtd[col] = pd.to_numeric(cur_mtd[col], errors="coerce")
                prev_mtd[col] = pd.to_numeric(prev_mtd[col], errors="coerce")

            def _tot(df):
                return (
                    df["Amount spent (USD)"].sum(),
                    df["Purchases conversion value"].sum(),
                    df["Purchases"].sum(),
                )

            spend_cur, rev_cur, conv_cur = _tot(cur_mtd)
            spend_prev, rev_prev, conv_prev = _tot(prev_mtd)

            roas_cur = rev_cur / spend_cur if spend_cur else 0
            roas_prev = rev_prev / spend_prev if spend_prev else 0

            cpa_cur = spend_cur / conv_cur if conv_cur else 0
            cpa_prev = spend_prev / conv_prev if conv_prev else 0

            return {
                "spend_cur": spend_cur,
                "spend_prev": spend_prev,
                "rev_cur": rev_cur,
                "rev_prev": rev_prev,
                "conv_cur": conv_cur,
                "conv_prev": conv_prev,
                "roas_cur": roas_cur,
                "roas_prev": roas_prev,
                "cpa_cur": cpa_cur,
                "cpa_prev": cpa_prev,
                "start_date": start_cur.strftime("%B %d"),
                "end_date": end_cur.strftime("%B %d"),
            }

        google_yoy = _summarize_google(google_cur_path, google_prev_path)
        meta_yoy = _summarize_meta(meta_cur_path, meta_prev_path)

        def _fmt(val: float, prefix: str = "$", digits: int = 0):
            if prefix:
                return f"{prefix}{val:,.{digits}f}"
            return f"{val:,.{digits}f}"

        if google_yoy and meta_yoy:
            end_label = google_yoy["end_date"]
            yoy_lines: list[str] = [
                f"\n## 5. Year-over-Year Growth (Week {google_yoy['start_date']}‚Äì{end_label})\n",
            ]

            def _yoy_rows(platform: str, data: dict[str, float]):
                rows: list[str] = []
                metrics = [
                    ("spend", "$", 0, "Spend"),
                    ("rev", "$", 0, "Revenue"),
                    ("conv", "", 0, "Conversions"),
                    ("roas", "", 2, "ROAS"),
                    ("cpa", "$", 2, "CPA"),
                ]
                rows.append(f"\n### {platform}\n")
                rows.append("| Metric | 2025 | 2024 | YoY Œî% |")
                rows.append("|-|-|-|-|")
                for key, prefix, digits, title in metrics:
                    cur_val = data[f"{key}_cur"]
                    prev_val = data[f"{key}_prev"]
                    pct = _pct_delta(cur_val, prev_val)
                    sign = "+" if pct > 0 else ("-" if pct < 0 else "")
                    delta_fmt = f"{sign}{abs(pct):.0f}%"
                    rows.append(
                        f"| {title} | {_fmt(cur_val, prefix, digits)} | {_fmt(prev_val, prefix, digits)} | {delta_fmt} |")
                return "\n".join(rows)

            yoy_lines.append(_yoy_rows("Google Ads", google_yoy))
            yoy_lines.append(_yoy_rows("Meta Ads", meta_yoy))

            yoy_section_md = "\n".join(yoy_lines) + "\n"

            final_report += yoy_section_md
    except Exception as e:
        print(f"‚ö†Ô∏è  YoY section failed: {e}")

    # -------------------------------------------------------------
    # üìù  Initialize the working markdown document
    #       Start with the base report produced by the core script and
    #       immediately append the Product & Category section we built
    #       above.  Subsequent steps will mutate this `final_report`
    #       string in-place (e.g., replace the old DTC table, inject
    #       WoW overview, add appendix).
    # -------------------------------------------------------------

    # Replace existing DTC section with new one
    # final_report = base_report + product_section_md

    # Label major base-report tables as Accrual Performance to distinguish from cash snapshot
    header_map = {
        "## 2. DTC Performance ‚Äî 7-Day Snapshot (Northbeam)": "## 2. DTC Breakdown (Accrual Performance) - 7 Days (Northbeam)",
        "## 3. Top Campaign Performance Analysis": "## 3. Top Campaign Performance Analysis (Accrual Performance)",
        "## 4. Channel Performance Metrics": "## 4. Channel Performance Metrics (Accrual Performance)",
        "### üí∞ Highest Spend Campaigns": "### üí∞ Highest Spend Campaigns",
    }
    for old, new in header_map.items():
        final_report = final_report.replace(old, new)

    # Append an appendix listing top unattributed campaigns for quick reference
    if not unattributed_df.empty:
        # Include all unattributed rows *with spend > 0*, sorted by spend descending
        top_unattributed = (
            unattributed_df[unattributed_df["spend"] > 0]
            .sort_values("spend", ascending=False)
        )
        appendix_lines = [
            "\n## Appendix: Top Unattributed Spend (Review for New Aliases)\n",
            "| Platform | Campaign | Ad Set | Ad | Spend | Revenue |",
            "|-|-|-|-|-|-|",
        ]

        # Helper to escape pipe characters that would break Markdown tables
        def _escape_pipes(text: str) -> str:
            """Return text safe for Markdown table cells by escaping pipe characters."""
            return str(text).replace("|", "\\|")

        for _, row in top_unattributed.iterrows():
            appendix_lines.append(
                f"| {row['breakdown_platform_northbeam']} | "
                f"{_escape_pipes(row['campaign_name'])[:40]} | "
                f"{_escape_pipes(row['adset_name'])[:30]} | "
                f"{_escape_pipes(row['ad_name'])[:30]} | "
                f"${row['spend']:,.0f} | ${row['attributed_rev']:,.0f} |")

        # Add totals row
        # Totals should reflect only the rows shown in the table
        tot_spend_un = top_unattributed['spend'].sum()
        tot_rev_un = top_unattributed['attributed_rev'].sum()
        appendix_lines.append(
            f"| **Totals** | ‚Äî | ‚Äî | ‚Äî | **${tot_spend_un:,.0f}** | **${tot_rev_un:,.0f}** |")

        final_report += "\n".join(appendix_lines)

    # ------------------------------------------------------------------
    # üóÑÔ∏è  Save report to dedicated output directory (reports/weekly)
    # ------------------------------------------------------------------
    REPORT_DIR = Path("reports/weekly")
    REPORT_DIR.mkdir(parents=True, exist_ok=True)

    out_file = REPORT_DIR / f"weekly-growth-report-with-products-{datetime.now().strftime('%Y-%m-%d')}.md"
    with open(out_file, "w", encoding="utf-8") as f:
        f.write(final_report)

    print(f"üìù Markdown report saved to {out_file}")


if __name__ == "__main__":
    main()