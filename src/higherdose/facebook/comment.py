"""Extract comments from Facebook ads.

Workflow:
1. Read Ad IDs (either from a file like data/facebook/ad-ids.txt or via the --ids CLI argument).
2. Use Marketing API to resolve each Ad's creative ‚Üí effective_object_story_id ‚Üí Post ID.
3. For each Post ID, fetch all comments (with pagination).
4. Persist everything to data/facebook/comments-YYYYMMDD-HHMMSS.json.

Requirements
------------
* A long-lived **user access token** that has the `ads_read` permission (used for Ad‚ÜíPost lookup).
  We load this from the latest tokens-*.json file generated by `higherdose.facebook.tokens`.
* Page access tokens (generated by the same tool) for comment lookup. If a post's Page ID
  isn't found in the token file, we fall back to the user token - this works as long as the 
  user token has `pages_read_user_content`.

Example
-------
$ python -m higherdose.facebook.comment \
    --ids-file data/facebook/ad-ids.txt \
    --output data/facebook
"""

import json
import sys
import argparse
import urllib.parse
import urllib.request
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional

from higherdose.utils.style import ansi
from higherdose.utils.logs import report
from higherdose.facebook.engine import TokenManager

logger = report.settings(__file__)

# Constants
CONFIG = Path("config", "facebook")
TOKENS_DIR = Path(CONFIG, "tokens")
DATA_FACEBOOK = Path("data", "facebook")
DEFAULT_IDS_FILE = Path(DATA_FACEBOOK, "ad-ids.txt")
GRAPH_VERSION = "v23.0"
BASE_URL = f"https://graph.facebook.com/{GRAPH_VERSION}"
CHUNK_SIZE = 50  # Graph API lets you batch ~50 IDs per request

#############################
# Utility helpers           #
#############################

def load_latest_tokens() -> Tuple[str, Dict[str, str]]:
    """Return (user_access_token, {page_id: page_token})."""
    tm = TokenManager()
    latest = tm.get_latest_run_file()
    if latest is None:
        print(f"{ansi.red}No token files found. Run `higherdose.facebook.tokens` first.{ansi.reset}")
        sys.exit(1)

    tm.load_run_data(latest)

    user_token = tm.user_config.long_lived_token.access_token if tm.user_config.long_lived_token else None
    if not user_token:
        print(f"{ansi.red}User long-lived token missing in {latest}{ansi.reset}")
        sys.exit(1)

    page_tokens = {}
    for page_id, cfg in tm.page_configs.items():
        if cfg.page_access_token and cfg.page_access_token.access_token:
            page_tokens[page_id] = cfg.page_access_token.access_token
    return user_token, page_tokens


def graph_request(endpoint: str, params: Dict[str, Any]) -> Dict[str, Any]:
    query = urllib.parse.urlencode(params)
    url = f"{BASE_URL}/{endpoint}?{query}"
    logger.info("Graph GET %s", url[:120] + ("‚Ä¶" if len(url) > 120 else ""))
    try:
        with urllib.request.urlopen(url) as resp:
            body = resp.read().decode()
            return json.loads(body)
    except urllib.error.HTTPError as e:
        err_body = e.read().decode(errors="ignore") if hasattr(e, "read") else ""
        logger.error("HTTP %s: %s ‚Äì %s", e.code, e.reason, err_body[:200])
        raise


def chunked(lst: List[str], size: int) -> List[List[str]]:
    for i in range(0, len(lst), size):
        yield lst[i:i + size]

#############################
# Core logic                #
#############################

def ad_ids_to_post_ids(ad_ids: List[str], user_token: str) -> Dict[str, str]:
    """Return mapping {ad_id: post_id}. Ignores ads that don't resolve."""
    mapping: Dict[str, str] = {}
    for chunk in chunked(ad_ids, CHUNK_SIZE):
        ids_str = ",".join(chunk)
        params = {
            "ids": ids_str,  # supplied via path later; we use base URL with ?ids=‚Ä¶
            "fields": "creative{effective_object_story_id}",
            "access_token": user_token,
        }
        data = graph_request("", params)  # blank endpoint when using ids param
        for ad_id, ad_data in data.items():
            creative = ad_data.get("creative", {})
            post_id = creative.get("effective_object_story_id")
            if post_id:
                mapping[ad_id] = post_id
            else:
                logger.warning("Ad %s ‚Äì no post_id", ad_id)
    return mapping


def fetch_all_comments(post_id: str, token: str) -> List[Dict[str, Any]]:
    """Return all comments for a post (depth-1)."""
    comments: List[Dict[str, Any]] = []
    params = {
        "fields": "id,from,message,created_time,like_count,user_likes,comment_count,permalink_url",
        "limit": 100,
        "access_token": token,
    }
    endpoint = f"{post_id}/comments"
    while True:
        data = graph_request(endpoint, params)
        comments.extend(data.get("data", []))
        paging = data.get("paging", {})
        next_url = paging.get("next")
        if not next_url:
            break
        # Parse next_url into new endpoint + params
        parsed = urllib.parse.urlparse(next_url)
        endpoint = parsed.path.lstrip("/")
        params = dict(urllib.parse.parse_qsl(parsed.query))
    return comments


def resolve_page_token(post_id: str, page_tokens: Dict[str, str], fallback_token: str) -> str:
    """Return a page token suitable for the post. Uses fallback if page not found."""
    # post_id may be "{pageId}_{postId}" or just numeric. Extract page prefix if present.
    if "_" in post_id:
        page_id, _ = post_id.split("_", 1)
        return page_tokens.get(page_id, fallback_token)
    # If format without underscore, we don't know page id ‚Äì fall back.
    return fallback_token


def main(argv: Optional[List[str]] = None):
    parser = argparse.ArgumentParser(description="Fetch comments for a list of Facebook Ad IDs ‚Üí Post IDs ‚Üí Comments")
    parser.add_argument("--ids-file", type=Path, default=DEFAULT_IDS_FILE, help="Text file containing one Ad ID per line")
    parser.add_argument("--output", type=Path, default=SAVE_DIR, help="Directory to write comments-*.json")
    args = parser.parse_args(argv)

    if not args.ids_file.exists():
        print(f"{ansi.red}IDs file {args.ids_file} not found{ansi.reset}")
        sys.exit(1)

    ad_ids = [line.strip() for line in args.ids_file.read_text().splitlines() if line.strip()]
    if not ad_ids:
        print("No Ad IDs provided.")
        sys.exit(1)

    print(f"üõ∞Ô∏è  Processing {ansi.yellow}{len(ad_ids)}{ansi.reset} Ad IDs‚Ä¶")

    user_token, page_tokens = load_latest_tokens()

    # 1. Ads ‚Üí Posts
    ad_to_post = ad_ids_to_post_ids(ad_ids, user_token)
    post_ids = list(ad_to_post.values())
    print(f"üîó Resolved {ansi.yellow}{len(post_ids)}{ansi.reset} Post IDs from {len(ad_ids)} ads.")

    # 2. Posts ‚Üí Comments
    all_comments: Dict[str, List[Dict[str, Any]]] = {}
    total_comments = 0
    for post_id in post_ids:
        token = resolve_page_token(post_id, page_tokens, user_token)
        comments = fetch_all_comments(post_id, token)
        all_comments[post_id] = comments
        total_comments += len(comments)
        print(f"üí¨ Post {ansi.cyan}{post_id}{ansi.reset}: fetched {ansi.yellow}{len(comments)}{ansi.reset} comments")

    # 3. Persist
    timestamp = datetime.now().strftime("%Y-%m-%d-%H%M%S")
    args.output.mkdir(parents=True, exist_ok=True)
    out_file = args.output / f"comments-{timestamp}.json"

    payload = {
        "metadata": {
            "created_at": datetime.now().isoformat(),
            "ad_count": len(ad_ids),
            "post_count": len(post_ids),
            "comment_count": total_comments,
        },
        "ad_to_post": ad_to_post,
        "comments": all_comments,
    }
    out_file.write_text(json.dumps(payload, indent=2, ensure_ascii=False))
    print(f"‚úÖ Saved results to {ansi.cyan}{out_file}{ansi.reset}")


if __name__ == "__main__":
    main()
