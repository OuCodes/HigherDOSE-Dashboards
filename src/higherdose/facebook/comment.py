"""Extract comments from Facebook ads.

Workflow:
1. Read Ad IDs (either from a file like data/facebook/ad-ids.txt or via the --ids CLI argument).
2. Use Marketing API to resolve each Ad's creative â†’ effective_object_story_id â†’ Post ID.
3. For each Post ID, fetch all comments (with pagination).
4. Persist everything to data/facebook/comments-YYYYMMDD-HHMMSS.json.

Requirements
------------
* A long-lived **user access token** that has the `ads_read` permission (used for Adâ†’Post lookup).
  We load this from the latest tokens-*.json file generated by `higherdose.facebook.tokens`.
* Page access tokens (generated by the same tool) for comment lookup. If a post's Page ID
  isn't found in the token file, we fall back to the user token - this works as long as the 
  user token has `pages_read_user_content`.

Example
-------
$ python -m higherdose.facebook.comment \
    --ids-file data/facebook/ad-ids.txt \
    --output data/facebook
"""

import json
import sys
import argparse
import urllib.parse
import urllib.request
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional

from higherdose.utils.style import ansi
from higherdose.utils.logs import report
from higherdose.facebook.engine import TokenManager

logger = report.settings(__file__)

# Constants
CONFIG = Path("config", "facebook")
TOKENS_DIR = Path(CONFIG, "tokens")
DATA_FACEBOOK = Path("data", "facebook")
DEFAULT_IDS_FILE = Path(DATA_FACEBOOK, "ad-ids.txt")
GRAPH_VERSION = "v23.0"
BASE_URL = f"https://graph.facebook.com/{GRAPH_VERSION}"
CHUNK_SIZE = 50  # Graph API lets you batch ~50 IDs per request

# HTTP settings
REQUEST_TIMEOUT = 30  # 30 seconds timeout
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds between retries
RATE_LIMIT_DELAY = 1  # seconds between requests to avoid rate limiting

#############################
# Utility helpers           #
#############################

def load_latest_tokens() -> Tuple[str, Dict[str, str]]:
    """Return (user_access_token, {page_id: page_token})."""
    tm = TokenManager()
    latest = tm.get_latest_run_file()
    if latest is None:
        error_msg = "No token files found. Run `higherdose.facebook.tokens` first."
        print(f"{ansi.red}{error_msg}{ansi.reset}")
        sys.exit(1)

    tm.load_run_data(latest)

    long_lived_token = tm.user_config.long_lived_token
    user_token = long_lived_token.access_token if long_lived_token else None
    if not user_token:
        print(f"{ansi.red}User long-lived token missing in {latest}{ansi.reset}")
        sys.exit(1)

    page_tokens = {}
    for page_id, cfg in tm.page_configs.items():
        if cfg.page_access_token and cfg.page_access_token.access_token:
            page_tokens[page_id] = cfg.page_access_token.access_token
    return user_token, page_tokens


def graph_request(endpoint: str, params: Dict[str, Any], retry_count: int = 0) -> Dict[str, Any]:
    """Make a Graph API request with error handling, timeout, and retry logic."""
    query = urllib.parse.urlencode(params)
    url = f"{BASE_URL}/{endpoint}?{query}"
    logger.info("Graph GET %s", url[:120] + ("â€¦" if len(url) > 120 else ""))

    print(f"\n{ansi.blue}DEBUG: Making Graph API request:{ansi.reset}")
    print(f"  URL: {url[:150]}{'...' if len(url) > 150 else ''}")
    if retry_count > 0:
        print(f"  Retry attempt: {retry_count}/{MAX_RETRIES}")

    try:
        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=REQUEST_TIMEOUT) as resp:
            body = resp.read().decode()
            print(f"  Response status: {ansi.green}{resp.status}{ansi.reset}")
            print(f"  Response body length: {len(body)} characters")
            body_preview = f"{body[:500]}{'...' if len(body) > 500 else ''}"
            print(f"  Response body (first 500 chars): {ansi.grey}{body_preview}{ansi.reset}")

            data = json.loads(body)

            # Check for Facebook errors in the response
            if 'error' in data:
                error = data['error']
                print(f"  {ansi.red}Facebook API Error:{ansi.reset}")
                print(f"    Message: {error.get('message', 'Unknown')}")
                print(f"    Type: {error.get('type', 'Unknown')}")
                print(f"    Code: {error.get('code', 'Unknown')}")

                # Handle rate limiting specifically
                if error.get('code') in [4, 17, 32]:  # Rate limit error codes
                    if retry_count < MAX_RETRIES:
                        delay = RETRY_DELAY * (2 ** retry_count)  # Exponential backoff
                        print(f"  {ansi.yellow}Rate limit detected, waiting {delay}s before retry...{ansi.reset}")
                        time.sleep(delay)
                        return graph_request(endpoint, params, retry_count + 1)
                    else:
                        raise Exception(f"Rate limit exceeded after {MAX_RETRIES} retries")

            return data

    except urllib.error.HTTPError as e:
        err_body = e.read().decode(errors="ignore") if hasattr(e, "read") else ""
        print(f"  {ansi.red}HTTP Error {e.code}: {e.reason}{ansi.reset}")
        error_preview = f"{err_body[:300]}{'...' if len(err_body) > 300 else ''}"
        print(f"  Error body: {ansi.red}{error_preview}{ansi.reset}")

        # Retry on server errors (5xx) or rate limits (429)
        if e.code in [429, 500, 502, 503, 504] and retry_count < MAX_RETRIES:
            delay = RETRY_DELAY * (2 ** retry_count)
            print(f"  {ansi.yellow}Retrying in {delay}s... (attempt {retry_count + 1}/{MAX_RETRIES}){ansi.reset}")
            time.sleep(delay)
            return graph_request(endpoint, params, retry_count + 1)

        logger.error("HTTP %s: %s â€“ %s", e.code, e.reason, err_body[:200])
        raise

    except (urllib.error.URLError, OSError, ConnectionError) as e:
        print(f"  {ansi.red}Network Error: {str(e)}{ansi.reset}")

        # Retry on network errors
        if retry_count < MAX_RETRIES:
            delay = RETRY_DELAY * (2 ** retry_count)
            print(f"  {ansi.yellow}Network error, retrying in {delay}s... (attempt {retry_count + 1}/{MAX_RETRIES}){ansi.reset}")
            time.sleep(delay)
            return graph_request(endpoint, params, retry_count + 1)

        logger.error("Network error after %d retries: %s", MAX_RETRIES, str(e))
        raise


def chunked(lst: List[str], size: int) -> List[List[str]]:
    """Split a list into chunks of specified size."""
    for i in range(0, len(lst), size):
        yield lst[i:i + size]

#############################
# Core logic                #
#############################

def ad_ids_to_post_ids(ad_ids: List[str], user_token: str) -> Dict[str, List[str]]:
    """Return mapping {ad_id: [post_id1, post_id2, ...]}. Returns all posts associated with each ad."""
    mapping: Dict[str, List[str]] = {}

    print(f"\n{ansi.cyan}DEBUG: Starting ad-to-post resolution for {len(ad_ids)} ads{ansi.reset}")
    print(f"{ansi.cyan}DEBUG: Will fetch ALL creatives per ad to capture multiple posts{ansi.reset}")

    chunks = list(chunked(ad_ids, CHUNK_SIZE))
    for chunk_idx, chunk in enumerate(chunks):
        # Add rate limiting delay between chunks (except for the first one)
        if chunk_idx > 0:
            print(f"\n{ansi.yellow}Rate limiting: waiting {RATE_LIMIT_DELAY}s before processing next chunk...{ansi.reset}")
            time.sleep(RATE_LIMIT_DELAY)

        ids_str = ",".join(chunk)
        # Request ALL creatives for each ad, not just the primary one
        params = {
            "ids": ids_str,
            "fields": "adcreatives{effective_object_story_id,object_story_id,object_id,name}",
            "access_token": user_token,
        }

        print(f"\n{ansi.yellow}DEBUG: Requesting chunk {chunk_idx + 1}/{len(chunks)} with {len(chunk)} ads:{ansi.reset}")
        for i, ad_id in enumerate(chunk, 1):
            print(f"  {i}. Ad ID: {ansi.cyan}{ad_id}{ansi.reset}")

        print(f"{ansi.yellow}DEBUG: API call params:{ansi.reset}")
        print(f"  fields: {params['fields']}")
        print(f"  ids: {ids_str}")

        try:
            data = graph_request("", params)  # blank endpoint when using ids param

            print(f"\n{ansi.green}DEBUG: API Response received:{ansi.reset}")
            print(f"  Response keys: {list(data.keys())}")

            for ad_id, ad_data in data.items():
                print(f"\n{ansi.cyan}Processing Ad ID: {ad_id}{ansi.reset}")
                print(f"  Raw ad_data: {json.dumps(ad_data, indent=2)}")

                # Get all creatives for this ad
                adcreatives = ad_data.get("adcreatives", {})
                creatives_data = adcreatives.get("data", [])

                print(f"  Found {ansi.yellow}{len(creatives_data)}{ansi.reset} creatives for this ad")

                post_ids = []
                for creative_idx, creative in enumerate(creatives_data, 1):
                    print(f"  \n  {ansi.magenta}Creative {creative_idx}:{ansi.reset}")
                    print(f"    Creative data: {json.dumps(creative, indent=4)}")

                    # Try multiple fields that might contain post IDs
                    potential_post_id = None

                    # Priority order: effective_object_story_id > object_story_id > object_id
                    if creative.get("effective_object_story_id"):
                        potential_post_id = creative["effective_object_story_id"]
                        print(f"    Found effective_object_story_id: {ansi.yellow}{potential_post_id}{ansi.reset}")
                    elif creative.get("object_story_id"):
                        potential_post_id = creative["object_story_id"]
                        print(f"    Found object_story_id: {ansi.yellow}{potential_post_id}{ansi.reset}")
                    elif creative.get("object_id"):
                        potential_post_id = creative["object_id"]
                        print(f"    Found object_id: {ansi.yellow}{potential_post_id}{ansi.reset}")
                    else:
                        print(f"    {ansi.red}No post ID found in this creative{ansi.reset}")
                        # Debug: show all available fields
                        print(f"    Available fields: {list(creative.keys())}")

                    if potential_post_id and potential_post_id not in post_ids:
                        post_ids.append(potential_post_id)
                        print(f"    {ansi.green}âœ“ Added to post list{ansi.reset}: {potential_post_id}")
                    elif potential_post_id:
                        print(f"    {ansi.yellow}âš  Duplicate post ID, skipping{ansi.reset}: {potential_post_id}")

                if post_ids:
                    mapping[ad_id] = post_ids
                    print(f"  {ansi.green}âœ“ Successfully mapped{ansi.reset}: {ad_id} â†’ {post_ids} ({len(post_ids)} posts)")
                else:
                    mapping[ad_id] = []
                    print(f"  {ansi.red}âœ— No post IDs found for this ad{ansi.reset}")
                    logger.warning("Ad %s â€“ no post IDs found in any creatives", ad_id)

        except Exception as e:
            print(f"\n{ansi.red}DEBUG: API request failed:{ansi.reset}")
            print(f"  Error: {str(e)}")
            print(f"  Ad IDs in this chunk: {chunk}")
            raise

    # Calculate totals
    total_posts = sum(len(posts) for posts in mapping.values())
    ads_with_multiple_posts = sum(1 for posts in mapping.values() if len(posts) > 1)

    print(f"\n{ansi.cyan}DEBUG: Ad-to-post resolution complete:{ansi.reset}")
    print(f"  Total ads processed: {len(ad_ids)}")
    print(f"  Total unique posts found: {ansi.yellow}{total_posts}{ansi.reset}")
    print(f"  Ads with multiple posts: {ansi.yellow}{ads_with_multiple_posts}{ansi.reset}")
    print(f"  Ads with no posts: {len([posts for posts in mapping.values() if len(posts) == 0])}")

    if mapping:
        print(f"  {ansi.green}Successful mappings:{ansi.reset}")
        for ad_id, post_list in mapping.items():
            if post_list:
                posts_str = ", ".join(post_list)
                print(f"    {ad_id} â†’ [{posts_str}] ({len(post_list)} posts)")
            else:
                print(f"    {ad_id} â†’ {ansi.red}[no posts]{ansi.reset}")

    return mapping


def fetch_all_comments(post_id: str, token: str) -> List[Dict[str, Any]]:
    """Return all comments for a post (depth-1). Returns empty list if permissions insufficient."""
    comments: List[Dict[str, Any]] = []
    params = {
        "fields": "id,from,message,created_time,like_count,user_likes,comment_count,permalink_url",
        "limit": 100,
        "access_token": token,
    }
    endpoint = f"{post_id}/comments"
    print(f"\n{ansi.blue}DEBUG: Fetching comments for post {post_id}{ansi.reset}")

    page_count = 0
    while True:
        # Add rate limiting delay between pages
        if page_count > 0:
            print(f"  {ansi.yellow}Rate limiting: waiting {RATE_LIMIT_DELAY}s before next page...{ansi.reset}")
            time.sleep(RATE_LIMIT_DELAY)

        try:
            data = graph_request(endpoint, params)
            page_count += 1

            # Debug the response
            if 'error' in data:
                error = data['error']
                print(f"  {ansi.red}âŒ Comments API Error:{ansi.reset}")
                print(f"    Message: {error.get('message', 'Unknown')}")
                print(f"    Code: {error.get('code', 'Unknown')}")

                # Check for permissions errors specifically
                if error.get('code') in [10, 200, 190] or 'permission' in error.get('message', '').lower():
                    print(f"  {ansi.yellow}âš ï¸  Permissions insufficient for this post, skipping...{ansi.reset}")
                    break
                else:
                    # Other API errors - also break to avoid infinite loop
                    break

            comments_batch = data.get("data", [])
            print(f"  {ansi.green}ðŸ“¥ Received {len(comments_batch)} comments in page {page_count}{ansi.reset}")
            if comments_batch:
                print(f"    First comment preview: {comments_batch[0].get('message', 'No message')[:100]}...")

            comments.extend(comments_batch)
            paging = data.get("paging", {})
            next_url = paging.get("next")
            if not next_url:
                break
            # Parse next_url into new endpoint + params
            parsed = urllib.parse.urlparse(next_url)
            endpoint = parsed.path.lstrip("/")
            params = dict(urllib.parse.parse_qsl(parsed.query))

        except urllib.error.HTTPError as e:
            # Handle HTTP errors gracefully (like 400 Bad Request for permissions)
            if e.code == 400:
                print(f"  {ansi.yellow}âš ï¸  HTTP 400 (Bad Request) - likely permissions issue, skipping post...{ansi.reset}")
                break
            elif e.code == 403:
                print(f"  {ansi.yellow}âš ï¸  HTTP 403 (Forbidden) - access denied, skipping post...{ansi.reset}")
                break
            else:
                # For other HTTP errors, re-raise to trigger retry logic
                raise
        except Exception as e:
            print(f"  {ansi.red}âŒ Unexpected error fetching comments: {str(e)}{ansi.reset}")
            break

    print(f"  {ansi.cyan}ðŸ“Š Total comments fetched: {len(comments)} across {page_count} pages{ansi.reset}")
    return comments


def resolve_page_token(post_id: str, page_tokens: Dict[str, str], fallback_token: str) -> str:
    """Return a page token suitable for the post. Uses fallback if page not found."""
    # post_id may be "{pageId}_{postId}" or just numeric. Extract page prefix if present.
    if "_" in post_id:
        page_id, _ = post_id.split("_", 1)
        return page_tokens.get(page_id, fallback_token)
    # If format without underscore, we don't know page id â€“ fall back.
    return fallback_token


def test_direct_post_access(post_id: str, token: str) -> bool:
    """Test if we can directly access a post ID to debug access issues."""
    print(f"\n{ansi.cyan}DEBUG: Testing direct access to post ID: {post_id}{ansi.reset}")

    try:
        params = {
            "fields": "id,message,created_time,from",
            "access_token": token,
        }
        data = graph_request(f"{post_id}", params)

        print(f"  {ansi.green}âœ“ Post accessible{ansi.reset}")
        print(f"  Post data: {json.dumps(data, indent=2)}")
        return True

    except (urllib.error.HTTPError, urllib.error.URLError, OSError,
            json.JSONDecodeError, KeyError) as e:
        print(f"  {ansi.red}âœ— Post not accessible: {str(e)}{ansi.reset}")
        return False


def main(argv: Optional[List[str]] = None):
    """Main function to orchestrate Facebook ad comments extraction process."""
    description = "Fetch comments for a list of Facebook Ad IDs â†’ Post IDs â†’ Comments"
    parser = argparse.ArgumentParser(description=description)
    ids_help = "Text file containing one Ad ID per line"
    parser.add_argument("--ids-file", type=Path, default=DEFAULT_IDS_FILE, help=ids_help)
    output_help = "Directory to write comments-*.json"
    parser.add_argument("--output", type=Path, default=DATA_FACEBOOK, help=output_help)
    parser.add_argument("--test-post", type=str, help="Test direct access to a specific post ID")
    args = parser.parse_args(argv)

    user_token, page_tokens = load_latest_tokens()

    # Test direct post access if requested
    if args.test_post:
        print(f"ðŸ§ª Testing direct post access for: {ansi.yellow}{args.test_post}{ansi.reset}")

        # Try with user token
        print(f"\n{ansi.blue}Testing with user token:{ansi.reset}")
        test_direct_post_access(args.test_post, user_token)

        # Try with page token if available
        if page_tokens:
            page_id, page_token = next(iter(page_tokens.items()))
            print(f"\n{ansi.blue}Testing with page token (Page ID: {page_id}):{ansi.reset}")
            test_direct_post_access(args.test_post, page_token)

        return

    if not args.ids_file.exists():
        print(f"{ansi.red}IDs file {args.ids_file} not found{ansi.reset}")
        sys.exit(1)

    ad_ids = [line.strip() for line in args.ids_file.read_text().splitlines() if line.strip()]
    if not ad_ids:
        print("No Ad IDs provided.")
        sys.exit(1)

    print(f"ðŸ›°ï¸  Processing {ansi.yellow}{len(ad_ids)}{ansi.reset} Ad IDsâ€¦")

    # 1. Ads â†’ Posts
    ad_to_posts = ad_ids_to_post_ids(ad_ids, user_token)

    # Flatten all post IDs while preserving uniqueness
    all_post_ids = []
    ad_to_post_mapping = {}  # Keep flat mapping for output compatibility

    for ad_id, post_list in ad_to_posts.items():
        for post_id in post_list:
            if post_id not in all_post_ids:
                all_post_ids.append(post_id)
            # For output compatibility, map ad to first post (most common case)
            if ad_id not in ad_to_post_mapping and post_list:
                ad_to_post_mapping[ad_id] = post_list[0]

    print(f"ðŸ”— Resolved {ansi.yellow}{len(all_post_ids)}{ansi.reset} unique Post IDs from {len(ad_ids)} ads.")

    # Show ads with multiple posts
    multi_post_ads = {ad_id: posts for ad_id, posts in ad_to_posts.items() if len(posts) > 1}
    if multi_post_ads:
        print(f"ðŸ“‹ {ansi.magenta}{len(multi_post_ads)}{ansi.reset} ads have multiple posts:")
        for ad_id, posts in multi_post_ads.items():
            print(f"  Ad {ad_id}: {len(posts)} posts â†’ {', '.join(posts)}")

    # 2. Posts â†’ Comments
    all_comments: Dict[str, List[Dict[str, Any]]] = {}
    total_comments = 0
    for i, post_id in enumerate(all_post_ids):
        # Add rate limiting delay between posts (except for the first one)
        if i > 0:
            print(f"\n{ansi.yellow}Rate limiting: waiting {RATE_LIMIT_DELAY}s before processing next post...{ansi.reset}")
            time.sleep(RATE_LIMIT_DELAY)

        token = resolve_page_token(post_id, page_tokens, user_token)
        # Debug: Show which token is being used
        if "_" in post_id:
            page_id = post_id.split("_", 1)[0]
            if page_id in page_tokens:
                token_type = f"page token (Page ID: {page_id})"
            else:
                token_type = "user token (fallback - page not found)"
        else:
            token_type = "user token (fallback - no page ID in post)"

        # Show which ads this post belongs to
        source_ads = [ad_id for ad_id, posts in ad_to_posts.items() if post_id in posts]
        ads_info = f"from {len(source_ads)} ad(s): {', '.join(source_ads[:3])}" + ("..." if len(source_ads) > 3 else "")

        print(f"ðŸ”‘ Using {ansi.yellow}{token_type}{ansi.reset} for post {post_id} ({i+1}/{len(all_post_ids)}) - {ads_info}")

        comments = fetch_all_comments(post_id, token)
        all_comments[post_id] = comments
        total_comments += len(comments)
        comment_msg = f"Post {post_id}: fetched {len(comments)} comments"
        print(f"ðŸ’¬ {ansi.cyan}{comment_msg}{ansi.reset}")

    # 3. Persist
    timestamp = datetime.now().strftime("%Y-%m-%d-%H%M%S")
    args.output.mkdir(parents=True, exist_ok=True)
    out_file = args.output / f"comments-{timestamp}.json"

    payload = {
        "metadata": {
            "created_at": datetime.now().isoformat(),
            "ad_count": len(ad_ids),
            "unique_post_count": len(all_post_ids),
            "comment_count": total_comments,
            "ads_with_multiple_posts": len(multi_post_ads),
        },
        "ad_to_posts": ad_to_posts,  # New: full mapping showing multiple posts per ad
        "ad_to_post": ad_to_post_mapping,  # Legacy: flat mapping for compatibility
        "comments": all_comments,
    }
    out_file.write_text(json.dumps(payload, indent=2, ensure_ascii=False))
    print(f"âœ… Saved results to {ansi.cyan}{out_file}{ansi.reset}")

    # Summary statistics
    print(f"\nðŸ“Š {ansi.cyan}Summary:{ansi.reset}")
    print(f"  â€¢ Processed {len(ad_ids)} ads")
    print(f"  â€¢ Found {len(all_post_ids)} unique posts")
    print(f"  â€¢ Collected {total_comments} total comments")
    print(f"  â€¢ {len(multi_post_ads)} ads had multiple posts")
    if total_comments > 0:
        avg_comments = total_comments / len([p for p in all_comments.values() if len(p) > 0])
        print(f"  â€¢ Average comments per post with comments: {avg_comments:.1f}")


if __name__ == "__main__":
    main()
